{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUST CLIMB ON THE DOOR, LEO!\n",
    "\n",
    "![leo](https://images2.minutemediacdn.com/image/upload/c_crop,h_347,w_620,x_0,y_36/f_auto,q_auto,w_1100/v1555428149/shape/mentalfloss/titanic_large.jpg)\n",
    "\n",
    "In this activity, we will use actual data from the ill-fated Titanic voyage. You are provided a file, `titanic.csv`, that contains information about passengers on the ship, as well as whether or not the person survived the sinking of the vessel.\n",
    "\n",
    "As you can probably imagine, several factors were critical in determining survival, such as gender, age and class. In this notebook, we will build a logistic regression classifier to see how effectively we can predict who survived the disaster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time around, we wrote our own implementation of gradient descent when performing linear regression. But in practice, it's more common (and preferable!) to use a highly optimized machine learning library that already implements the most common algorithms, so that we can focus on the problem to be solved. In this workbook, we'll use the `scikit-learn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn is the name of the package; we're importing some specific functions and classes that \n",
    "# we'll use further below.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "data = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# take a look at the data \n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our targets\n",
    "y = data[\"Survived\"]\n",
    "\n",
    "# recoding the gender column to be numeric (unfortunately, the historic dataset uses a limited, \n",
    "# binary notion of gender identity)\n",
    "data['Recoded_Gender'] = data['Gender'].apply(lambda g: 1 if g == 'female' else 0)\n",
    "# the apply() method is a good one to be familiar with, see documentation -->\n",
    "#   https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html\n",
    "# talk to us if you have questions!\n",
    "\n",
    "# build the feature matrix -- we'll only use three features in building our model\n",
    "# confused about the double brackets? talk to us!\n",
    "X = data[['Fare', 'Recoded_Gender', 'Age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for some additional data preprocessing: we're going to perform a train-test split (to get an unbiased estimate of model performance, as discussed in the videos) and then rescale the features (to make the optimizer's job easier in finding the best-fit parameters). Note that we perform the split _first_, before doing the scaling --- that way, we ensure that we don't leak _any_ information from our test fold into our training fold, including the means and standard deviations of our various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple usage of the train_test_split function, to divide our data into a\n",
    "# training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Answer the following questions based on the documentation for this function:\n",
    "#   https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "#   - Does the data get randomly shuffled before splitting?\n",
    "#   - What fraction of the data becomes training data and what fraction becomes test data?\n",
    "# Finally: modify the function call above so that you get an 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler objects in sklearn can be used to rescale data in various ways\n",
    "# in this case, we're using z-score based scaling\n",
    "# another common scaler is the MinMaxScaler which performs linear scaling into the range [0, 1]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# all scalers implement a fit and a transform method\n",
    "#   - fit: computes the appropriate statistics from the data\n",
    "#   - transform: rescales the data using the statistics computed by fit\n",
    "#   - fit_transform: a convenience method that applies fit(), followed by transform()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Questions to answer:\n",
    "#   - why did we call fit_transform on the training data but just transform on the test data?\n",
    "#   - why did we not call anything on y_train or y_test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# every model family in scikit-learn implements a fit and a predict method:\n",
    "#   - fit: takes the training data and targets and fits the model parameters to it\n",
    "#   - predict: takes a fitted model and makes predictions on the supplied data\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# at this point, we now have y_test (the true labels) and y_preds (our model's\n",
    "# predictions for those same data points); these two should have the same\n",
    "# dimensions\n",
    "print(y_test.shape)\n",
    "print(y_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to compute some metrics: first, let's check our accuracy\n",
    "print(\"Acc:\", accuracy_score(y_test, y_preds))\n",
    "print()\n",
    "\n",
    "# now, the F1 score\n",
    "print(\"F1:\", f1_score(y_test, y_preds))\n",
    "print()\n",
    "\n",
    "# we can also see the confusion matrix\n",
    "print(confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we created a logistic regression model by instantiating a `LogisticRegression` object. This uses some sensible default set of hyperparameters, but you will often want to customize these hyperparameters to your specific dataset. You can set these hyperparameters by passing arguments to the `LogisticRegression` constructor. For example, what if you wanted to change the strength of the regularization applied to the loss function? Identify which argument controls this by reading the documentation page:  \n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  \n",
    "Then, retrain a `LogisticRegression` model with a regularization strength of 0.01 and measure its performance using the F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to identify the best setting of this regularization strength hyperparameter. As noted in the videos, we need to be a little careful when doing this: you don't want to use the test set for determining the best hyperparameter setting, as you would be \"contaminating\" the test set, i.e., using it as part of the model fitting process. To ensure the integrity of the test set, we should instead split the training set further -- into a training set and a validation set. We can train a variety of `LogisticRegression` models, with different regularization strengths, on this smaller training set, and evaluate performance on the validation set. Then, once we've identified the best performing hyperparameter value, we can train a \"fresh\" model on the entire training fold using this value and measure its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll write code to implement the above workflow here\n",
    "# first, split the training data further into a training and validation set\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a range of LogisticRegression models, using different hyperparameter settings\n",
    "# on just the training set (use a loop); evaluate each model on the validation set\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of your results: F1 score vs. C value\n",
    "# remember to label your axes!\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally: what's the best C value based on your plot? train a *single* LogisticRegression model \n",
    "# on the entire training data using that setting -- how well does this model do on the test set?\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><p style='text-align: right;'> <b>Authors:</b> Michelle Kuchera, Raghuram Ramanujan </p></i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
