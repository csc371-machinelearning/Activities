{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in Keras\n",
    "\n",
    "Previously, we hand wrote a neural network using JAX for computing gradients using automatic differentiation.\n",
    "\n",
    "`tensorflow` (from google) or `pytorch` (facebook) are other packages used to build neural networks. `Keras` is a package that is built on top of all three of these package to allow very easy building and training of neural networks.\n",
    "\n",
    "Today, we will use `keras` buily on `tensorflow` to build a neural network. \n",
    "\n",
    "To get used to `keras`, let's first rebuild our `JAX` activity's neural network. I wrote this for you, but look over it carefully and play around with changing some parameters to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lots of imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is simply an alias for convenience\n",
    "layers = tf.keras.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a useful function\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    \"\"\"Plots a learning curve from a training history.\n",
    "    \n",
    "    Arguments:\n",
    "        history (dict): The training history returned by `model.fit()`.\n",
    "        \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(11, 6), dpi=100)\n",
    "    plt.plot(history.history['loss'], 'o-', label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], 'o:', color='r', label='Validation Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    #plt.xticks(range(0, len(history.history['loss'])), range(1, len(history.history['loss']) + 1))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create same synthetic data as last activity\n",
    "\n",
    "N = 1200 # number of examples\n",
    "n_feat = 3\n",
    "x = np.random.uniform(size=(N,n_feat))\n",
    "\n",
    "def true(x):\n",
    "    return .5*x[:,0] + .2*x[:,1] + 2*x[:,2] + np.random.normal(scale=.1)\n",
    "\n",
    "y = true(x)\n",
    "\n",
    "# my same janky train/val split\n",
    "split = int((N*.8)//1)\n",
    "train_x = x[:split]\n",
    "train_y = y[:split]\n",
    "test_x = x[split:]\n",
    "test_y = y[split:]\n",
    "split2=int((len(train_x[:,0]*.2)//1))\n",
    "val_x = x[:split2]\n",
    "val_y = y[:split2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the important part.. building the network\n",
    "\n",
    "# Build a simple neural network with one hidden layer usign keras\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Input(shape=(3,)))\n",
    "#model.add(layers.Input(input_shape=3))\n",
    "\n",
    "# Add a hidden layer with 4 nodes and sigmoid activation\n",
    "model.add(layers.Dense(4, activation='sigmoid'))\n",
    "\n",
    "# Add an output layer with a single node and linear activation for regression\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# That was easier, right?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, compile the model with lass and backprop details\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "model.compile(optimizer=opt, loss='MSE')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_x, train_y, epochs=100, validation_data=(val_x, val_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "plot_learning_curve(history)\n",
    "\n",
    "# plot predictions vs true values for the test set. Your model should work well!\n",
    "y_pred = model.predict(test_x)\n",
    "plt.plot(test_y,y_pred,'.')\n",
    "plt.plot([0,2.5],[0,2.5])\n",
    "plt.xlabel(\"y_true\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "We will now use `keras` in `tensorflow` to build an autoencoder. \n",
    "\n",
    "We will start with a simple neural network architecture that is composed of an input layer a lower-dimensional latent space, and an output layer of equal size.\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-06-at-3.17.13-PM.png\" width=\"400\" />\n",
    "\n",
    "Autoencoders are an *unsupervised learning* method. We will begin by using an autoencoder to create a latent space representation of the `digits` dataset, a reduced-dimension version of the `MNIST` dataset. Replacing the `digits` dataset with the larger `MNIST` dataset is perhaps a more useful activity, but increases the runtime of the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Again, we begin by loading our data, normalizing it, and putting it into he approporate format for our model. In this case, we need 1D arrays for our fully connected architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features:\\n   Shape: {}\\n   Type: {}\\n'.format(x_train.shape, x_train.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As always, we should rescale the data. Here, I rescaled the images to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the autoencoder. Ours is a standard feed-forward neural network architecture with three layers as descibed above.\n",
    "\n",
    "Let's start by reducing our dimensionality by a factor of two and see if we can recover our original images.\n",
    "\n",
    "One way to build an autoencoder is to store each layer into a variable so that we can access the different pieces later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "#input layer\n",
    "input_img = layers.Input(#complete me)\n",
    "#latent layer\n",
    "latent_layer = layers.Dense(#complete me, activation=\"??\")(input_img)\n",
    "# we also want to use this as an input once trained, no need to modify this\n",
    "latent_input = layers.Input(shape=(latent_dim,))\n",
    "# output layer\n",
    "output_layer = layers.Dense(#complete me, activation=\"??\")(latent_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we build the encoder and decoder from the same layers.\n",
    "\n",
    "Let's start with the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = tf.keras.models.Model(input_img, latent_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the decoder. This requires slightly more work because we want to have a latent representation as an Input in order to use the decoder as a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps from a latent space to a reconstructed output\n",
    "decoder = tf.keras.models.Model(latent_input, output_layer)\n",
    "\n",
    "\n",
    "# put the layers together to create your Model\n",
    "autoencoder = tf.keras.models.Model(input_img, decoder(encoder(input_img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr = 1e-4)\n",
    "autoencoder.compile(optimizer=opt, loss='MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, train!\n",
    "\n",
    "The follow training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = autoencoder.fit(x_train, x_train,\n",
    "                epochs=10, batch_size=512, # batch size can speed up this calculation\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode digits from the test det\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    #plt.imshow(x_test[i].reshape(28, 28))\n",
    "    #plt.gray()    \n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    #plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    #plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try k-means clustering on the latent space to see if the latent space can seperate out the digits in the test set.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a selection of test examples from various clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
