{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "\n",
    "\n",
    "Recall that our goal in support vector machines is to find the hyperplane that maximally separates the classes. The hyperplane is found by finding the support vectors, i.e. the training examples that lie on parallel planes that maximally separate the classes. The dividing hyperplane lies halfway between these two hyperplanes.\n",
    "\n",
    "For this example, our hyperplane is merely a line because we will deal with two dimensional data.\n",
    "\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/support-vector-machine-3.png\" width=400 />\n",
    "\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/support-vector-machine-5.png\" width=400 />\n",
    "\n",
    "\n",
    "recall our hyperplane equation for two features $x_1$ and $x_2$:\n",
    "\n",
    "$$w_1x_1 + w_2x_2 + c = 0$$\n",
    "\n",
    "this can also be written as\n",
    "\n",
    "$$\\vec{w}\\cdot\\vec{x} + c = 0$$\n",
    "\n",
    "which is equivalent to the equation for a line as we saw in class last week.\n",
    "\n",
    "Parallel planes have the same coefficients but are offset by a constant (which we choose as $+1$ and $-1$ here):\n",
    "\n",
    "$$w_1x_1 + w_2x_2 + c = +1$$\n",
    "$$w_1x_1 + w_2x_2 + c = -1$$\n",
    "\n",
    "we define $$y_+(\\vec{w}\\cdot\\vec{x} + c) = +1$$\n",
    "\n",
    "$$y_-(\\vec{w}\\cdot\\vec{x} + c) = -1$$\n",
    "\n",
    "or $$\\mathrm{sign}(\\vec{w}\\cdot\\vec{x} + c) - 1 = 0$$\n",
    "\n",
    "\n",
    "$$margin = \\frac{2}{\\|w \\|}$$\n",
    "\n",
    "Therefore maximizing the margin means minimizing $\\|w\\|$. Minimizing $\\|w\\|$ also means minimizing $\\|w\\|^2$, which is great because that is convex!\n",
    "\n",
    "Therefore, we can minimize $\\|w\\|^2$ while maintaining the constraint that the training data must entirely lie outside of the parallel separating hyperplanes $$\\mathrm{sign}(\\vec{w}\\cdot\\vec{x} + c) - 1 \\gt 0$$.\n",
    "\n",
    "This is a *constrained* optimization problem. We will not solve this rigorously here, we will find our coefficients $\\vec{w}$ and $c$ in a more intuitive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data! Note that this will generate different blobs each time you run this.\n",
    "# You can seed \n",
    "X,y =  make_blobs(n_samples=50,n_features=2,centers=2,cluster_std=1.05)\n",
    "\n",
    "# change features to -1 and 1 instead of 0 and 1 for nice math\n",
    "y = np.where(y==0, -1, y)\n",
    "\n",
    "# plot data to verify that blobs are perfectly seperable\n",
    "plt.scatter(X[:,0],X[:,1],marker='o',c=y)\n",
    "plt.show()\n",
    "\n",
    "# normalize data to make our janky optimization easier\n",
    "mean1, mean2 = np.mean(X[:,0]), np.mean(X[:,1])\n",
    "std1, std2 = np.std(X[:,0]), np.std(X[:,1])\n",
    "#print(mean1, mean2, std1, std2)\n",
    "X[:,0] = (X[:,0]-mean1)/std1\n",
    "X[:,1] = (X[:,1]-mean2)/std2\n",
    "\n",
    "# plot to make sure nothing is wrong with the normalization\n",
    "plt.scatter(X[:,0],X[:,1],marker='o',c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below, define the function that predicts values for a hyperplane \n",
    "# defined by w and c\n",
    "\n",
    "def predict(features, w, c):\n",
    "    # sign( x dot w + c )\n",
    "    y_pred = np.sign(np.dot(np.array(features),w)+c)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the janky optimization.\n",
    "\n",
    "We will take a brute force approach and test many hyperplanes, defined by $\\vec{w}$ and $c$, and find which of the combinations separate the data with the largest margin.\n",
    "\n",
    "We are going to step down in $\\vec{w}$, keeping track of smallest one that separated the data. Once we have the smallest successful $\\vec{w}$, we will step down again with a smaller stepsize to see if we can find a better solution.\n",
    "\n",
    "The process is as follows:\n",
    " - Loop from a large stepsize to a small stepsize\n",
    " - For the given step size, loop through  various $(w_1, w_2)$ combinations.\n",
    " - For each $\\vec{w}$ that we test, we will also test  $\\vec{w}$ vectors that have the same component magnitudes (therefore $\\| w\\|$ is the same), but point in a different direction.  For example, we would test $\\vec{w} = (2,3), (-2,3), (2,-3), (-2, -3)$.\n",
    " - For each permutation that we try, test to see if the $(w_1, w_2,c)$ hyperplane perfectly seperates the data. If so, save it as the smallest $\\| w\\|$ so far.\n",
    "\n",
    "\n",
    "Notes on ranges for values:\n",
    " - Starting with a max w of $(5,5)$ worked fine for me for most blobs\n",
    " - I tested $c$'s spanning from $-10$ to $10$, but since the data is normalized, $c$ was often very near zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below, we will follow the process outlined above\n",
    "\n",
    "\n",
    "\n",
    "def fit(data):\n",
    "    \n",
    "\n",
    "    \n",
    "    # if your data is normalized, these stepsizes should work\n",
    "    for wstep in [1.,.1,.01,.001, 0.0001]:\n",
    "        # below loop over various w1 and w2s:\n",
    "     \n",
    "                # scan potential c's\n",
    "              \n",
    "                    # scan all vector directions of w\n",
    "                   \n",
    "                       \n",
    "                        # do we separate data with this hyperplane?\n",
    "                        \n",
    "\n",
    "                        # if so, it's the largest margin so far\n",
    "                        # so save it!\n",
    "\n",
    "\n",
    "    return w_best, c_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find your best hyperplane:\n",
    "\n",
    "wf, cf = fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting hyperplane:\n",
    "wf, cf = fit(X)\n",
    "\n",
    "print(wf,cf)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "xs = np.linspace(min(X[:,0]),max(X[:,0]),100)\n",
    "m = -wf[0]/wf[1]\n",
    "b = cf/wf[1]\n",
    "print(m,b)\n",
    "plt.plot(xs,m*xs+b)\n",
    "plt.ylim(-2,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the predictive power of your fit\n",
    "predict(np.array([1,-1.5]),wf, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
